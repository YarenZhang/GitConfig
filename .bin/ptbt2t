#!/bin/bash
# d for using tfdbg
# p for using all ipdb breakpoints in python file
if [[ $1 == "d" ]]
then
    DEBUG="--tfdbg"
else
    DEBUG=""
fi

if [[ $1 == "p" ]]
then
    export t2tdbg="True"
else
    export t2tdbg=""
fi

PROBLEM=languagemodel_ptb10k
MODEL=transformer
HPARAMS=transformer_small
# MODEL=lstm_seq2seq_attention
# HPARAMS=lstm_attention
# MODEL=custom_model

USER_DIR=$HOME/ptb/t2t_usr
# MODEL-HPARAMS format is mandatory
TRAIN_DIR=$HOME/ptb/t2t_train/$PROBLEM/$MODEL-$HPARAMS
TMP_DIR=/tmp/t2t_datagen
#DATA_DIR=$HOME/ptb/t2t_data

# rm -rf $TRAIN_DIR
# mkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR $USER_DIR

# # Generate data
# t2t-datagen \
#   --data_dir=$DATA_DIR \
#   --tmp_dir=$TMP_DIR \
#   --problem=$PROBLEM
# #   --t2t_usr_dir=$USER_DIR \

# Train
# *  If you run out of memory, add --hparams='batch_size=1024'.
t2t-trainer \
  --data_dir=$DATA_DIR \
  --output_dir=$TRAIN_DIR \
  --t2t_usr_dir=$USER_DIR \
  --problems=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  ${DEBUG}

# # Decode
# DECODE_FILE=$DATA_DIR/decode_this.txt
# echo "Hello world" >> $DECODE_FILE
# echo "Goodbye world" >> $DECODE_FILE
# echo -e 'Hallo Welt\nAuf Wiedersehen Welt' > ref-translation.de

# BEAM_SIZE=4
# ALPHA=0.6

# t2t-decoder \
#   --data_dir=$DATA_DIR \
#   --output_dir=$TRAIN_DIR \
#   --problems=$PROBLEM \
#   --model=$MODEL \
#   --hparams_set=$HPARAMS \
#   --decode_hparams="beam_size=$BEAM_SIZE,alpha=$ALPHA" \
#   --decode_from_file=$DECODE_FILE \
#   --decode_to_file=translation.en
# #   --t2t_usr_dir=$USER_DIR \

# # See the translations
# cat translation.en

# # Evaluate the BLEU score
# # Note: Report this BLEU score in papers, not the internal approx_bleu metric
# t2t-bleu --translation=translation.en --reference=ref-translation.de

